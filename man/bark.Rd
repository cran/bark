% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bark.r
\name{bark}
\alias{bark}
\title{Nonparametric Regression using Bayesian Additive Regression Kernels}
\usage{
bark(
  formula,
  data,
  subset,
  na.action = na.omit,
  testdata = NULL,
  selection = TRUE,
  common_lambdas = TRUE,
  classification = FALSE,
  keepevery = 100,
  nburn = 100,
  nkeep = 100,
  printevery = 1000,
  keeptrain = FALSE,
  verbose = FALSE,
  fixed = list(),
  tune = list(lstep = 0.5, frequL = 0.2, dpow = 1, upow = 0, varphistep = 0.5, phistep =
    1),
  theta = list()
)
}
\arguments{
\item{formula}{model formula for the model with all predictors,
Y ~ X.  THe X variables will be centered and scaled as part of model fitting.}

\item{data}{a data frame.  Factors will be converted to numerical vectors based on
the using `model.matrix`.}

\item{subset}{an optional vector specifying a subset of observations to be
used in the fitting process.}

\item{na.action}{a function which indicates what should happen when the data
contain NAs. The default is "na.omit".}

\item{testdata}{Dataframe with test data for out of sample prediction.\cr
Should have same structure as data.}

\item{selection}{Logical variable indicating whether variable 
dependent kernel parameters \eqn{\lambda} may be set to zero in the MCMC; 
default is TRUE. \cr}

\item{common_lambdas}{Logical variable indicating whether
kernel parameters \eqn{\lambda} should be predictor specific or common across
predictors;  default is TRUE.   Note if  \emph{common_lambdas = TRUE} and 
\emph{selection = TRUE} this applies just to the non-zero \eqn{lambda_j}. \cr}

\item{classification}{TRUE/FALSE logical variable,
indicating a classification or regression problem.}

\item{keepevery}{Every keepevery draw is kept to be returned to the user}

\item{nburn}{Number of MCMC iterations (nburn*keepevery)
to be treated as burn in.}

\item{nkeep}{Number of MCMC iterations kept for the posterior inference.\cr
nkeep*keepevery iterations after the burn in.}

\item{printevery}{As the MCMC runs, a message is printed every printevery draws.}

\item{keeptrain}{Logical, whether to keep results for training samples.}

\item{verbose}{Logical, whether to print out messages}

\item{fixed}{A list of fixed hyperparameters, using the default values if not
specified.\cr
alpha = 1: stable index, must be 1 currently.\cr
eps = 0.5: approximation parameter.\cr
gam = 5: intensity parameter.\cr
la = 1: first argument of the gamma prior on kernel scales.\cr
lb = 2: second argument of the gamma prior on kernel scales.\cr
pbetaa = 1: first argument of the beta prior on plambda.\cr
pbetab = 1: second argument of the beta prior on plambda.\cr
n: number of training samples, automatically generates.\cr
p: number of explanatory variables, automatically generates.\cr
meanJ: the expected number of kernels, automatically generates.}

\item{tune}{A list of tuning parameters, not expected to change.\cr
lstep: the stepsize of the lognormal random walk on lambda.\cr
frequL: the frequency to update L.\cr
dpow: the power on the death step.\cr
upow: the power on the update step.\cr
varphistep: the stepsize of the lognormal random walk on varphi.\cr
phistep: the stepsize of the lognormal random walk on phi.}

\item{theta}{A list of the starting values for the parameter theta,
use defaults if nothing is given.}
}
\value{
\code{bark} returns a list, including:
 \item{call}{the matched call}
 \item{fixed}{Fixed hyperparameters}
 \item{tune}{Tuning parameters used}
 \item{theta.last}{The last set of parameters from the posterior draw}
 \item{theta.nvec}{A matrix with nrow(x.train)\eqn{+1} rows and (nkeep) columns,
recording the  number of kernels at each training sample}
 \item{theta.varphi}{ A matrix with nrow(x.train)
 \eqn{+1} rows and (nkeep) columns,
 recording the precision in the normal gamma prior
 distribution for the regression coefficients}
 \item{theta.beta}{A matrix with nrow(x.train)\eqn{+1} rows and (nkeep) columns,
 recording the regression coefficients}
 \item{theta.lambda}{A matrix with ncol(x.train) rows and (nkeep) columns,
  recording the kernel scale parameters}
 \item{thea.phi}{The vector of length nkeep,
 recording the precision in regression Gaussian noise
 (1 for the classification case)}
 \item{yhat.train}{A matrix with nrow(x.train) rows and (nkeep) columns.
 Each column corresponds to a draw \eqn{f^*}{f*} from
 the posterior of \eqn{f}
  and each row corresponds to a row of x.train.
 The \eqn{(i,j)} value is \eqn{f^*(x)}{f*(x)} for
 the \eqn{j^{th}}{j\^th} kept draw of \eqn{f}
 and the \eqn{i^{th}}{i\^th} row of x.train.\cr
 For classification problems, this is the value
 of the expectation for the underlying normal
 random variable.\cr
 Burn-in is dropped}
\item{yhat.test}{Same as yhat.train but now the x's
are the rows of the test data}
\item{yhat.train.mean}{train data fits = row mean of yhat.train}
\item{yhat.test.mean}{test data fits = row mean of yhat.test}
}
\description{
BARK is a Bayesian \emph{sum-of-kernels} model.\cr
For numeric response \eqn{y}, we have
\eqn{y = f(x) + \epsilon}{y = f(x) + e},
where \eqn{\epsilon \sim N(0,\sigma^2)}{e ~ N(0,sigma\^2)}.\cr
For a binary response \eqn{y}, \eqn{P(Y=1 | x) = F(f(x))},
where \eqn{F}
denotes the standard normal cdf (probit link).
\cr
In both cases, \eqn{f} is the sum of many Gaussian kernel functions.
The goal is to have very flexible inference for the unknown
function \eqn{f}.
BARK uses an approximation to a Cauchy process as the prior distribution
for the unknown function \eqn{f}.

Feature selection can be achieved through the inference
on the scale parameters in the Gaussian kernels.
BARK accepts four different types of prior distributions,
\emph{e}, \emph{d}, enabling
either soft shrinkage or  \emph{se}, \emph{sd}, enabling hard shrinkage for the scale
parameters.
}
\details{
BARK is implemented using a Bayesian MCMC method.
At each MCMC interaction, we produce a draw from the joint posterior
distribution, i.e. a full configuration of regression coefficients,
kernel locations and kernel parameters etc.

Thus, unlike a lot of other modelling methods in R,
we do not produce a single model object
from which fits and summaries may be extracted.
The output consists of values
\eqn{f^*(x)}{f*(x)} (and \eqn{\sigma^*}{sigma*} in the numeric case)
where * denotes a particular draw.
The \eqn{x} is either a row from the training data (x.train)
}
\examples{
##Simulated regression example
# Friedman 2 data set, 200 noisy training, 1000 noise free testing
# Out of sample MSE in SVM (default RBF): 6500 (sd. 1600)
# Out of sample MSE in BART (default):    5300 (sd. 1000)
traindata <- data.frame(sim_Friedman2(200, sd=125))
testdata <- data.frame(sim_Friedman2(1000, sd=0))
fit.bark.d <- bark(y ~ ., data=traindata, testdata= testdata,
                   nburn=10, nkeep=100, keepevery=10,
                   classification=FALSE, 
                   common_lambdas = FALSE,
                   selection = FALSE)
boxplot(data.frame(fit.bark.d$theta.lambda))
mean((fit.bark.d$yhat.test.mean-testdata$y)^2)
\dontrun{
 ##Simulate classification example
 # Circle 5 with 2 signals and three noisy dimensions
 # Out of sample erorr rate in SVM (default RBF): 0.110 (sd. 0.02)
 # Out of sample error rate in BART (default):    0.065 (sd. 0.02)
 traindata <- sim_Circle(200, dim=5)
 testdata <- sim_Circle(1000, dim=5)
 fit.bark.se <- bark(y ~ ., data=data.frame(traindata), 
                     x.test= data.frame(testdata), 
                     classification=TRUE)
 boxplot(as.data.frame(fit.bark.se$theta.lambda))
 mean((fit.bark.se$yhat.test.mean>0)!=testdata$y)
}
}
\references{
Ouyang, Zhi (2008) Bayesian Additive Regression Kernels.
Duke University. PhD dissertation, page 58.
}
\seealso{
Other bark functions: 
\code{\link{bark-package-deprecated}},
\code{\link{bark-package}},
\code{\link{sim_Friedman1}()},
\code{\link{sim_Friedman2}()},
\code{\link{sim_Friedman3}()},
\code{\link{sim_circle}()}
}
\concept{bark functions}
